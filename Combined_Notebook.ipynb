{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02807 Computational Tools for Data Science\n",
    "##  Book recomendation system \n",
    "### Introduction\n",
    "The notebook first loads two dataframes, one containing data for each book, and one containing reviews of the books.\n",
    "\n",
    "Then the data is cleaned.\n",
    "\n",
    "Furthermore, different methods are used to recommend books.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a> <br>\n",
    " # Table of Contents  \n",
    "1. [Data](#data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env VARIABLE=comptools-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a> <br>\n",
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = pd.read_csv('data/books_data.csv') #, nrows=1000)\n",
    "print(f\"Books data has \" + str(books_data.columns.size) + \" columns with \" + str(books_data.shape[0]) + \" rows each.\")\n",
    "books_ratings = pd.read_csv('data/books_rating.csv') #, nrows=1000)\n",
    "print(f\"Books ratings has \" + str(books_ratings.columns.size) + \" columns with \" + str(books_ratings.shape[0]) + \" rows each.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "The following section cleans the data. \n",
    "This is done with the following steps\n",
    "\n",
    "- We remove columns that we do not use and rename some of the columns.\n",
    "- We change \"&\" to \"and\", remove special characters in the titles, and make titles lower case. This is done for both data sets.\n",
    "- We remove dublicates of titles and descriptions.\n",
    "- We remove duplicate reviews where same user reviews same book multiple times.\n",
    "- We then remove reviews where title is NaN.\n",
    "- Remove books with less than 10 reviews.\n",
    "- Remove users with less than 20 reviews.\n",
    "- Add column to book_data: average_rating and helpfullness of reviews to books_ratings\n",
    "\n",
    "The data cleaning is now complete and we can go to the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titles\n",
    "books_data_1 = books_data.drop(['image', 'previewLink', 'infoLink', 'publisher', 'ratingsCount'], axis=1, inplace=False)\n",
    "books_ratings_1 = books_ratings.drop(['Id','Price', 'profileName', 'review/time'], axis=1, inplace=False)\n",
    "\n",
    "books_data_1.rename(columns={'Title': 'title','publishedDate': 'published_date'}, inplace=True)\n",
    "books_ratings_1.rename(columns={'Title': 'title','User_id':'user_id',  'review/score': 'score','review/helpfulness': 'helpfulness', 'review/text': 'text', 'review/summary': 'summary'}, inplace=True)\n",
    "print(f\"Books data now only has \" + str(books_data_1.columns.size) + \" columns - \" + str(list(books_data_1.columns)))\n",
    "print(f\"Books ratings now only has \" + str(books_ratings_1.columns.size) + \" columns - \" + str(list(books_ratings_1.columns))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes all titles lowercase; then removes any part of the title that includes a parenthesis, and replaces any ampersands with 'and'\n",
    "books_data_1['title'] = books_data_1['title'].map(lambda s: s.lower() if type(s) == str else s)\n",
    "books_data_1['title'] = books_data_1['title'].replace(r\"\\(.*\\)\",\"\", regex=True)\n",
    "books_data_1['title'] = books_data_1['title'].replace(r\"\\&\",\"and\", regex=True)\n",
    "\n",
    "books_ratings_1['title'] = books_ratings_1['title'].map(lambda s: s.lower() if type(s) == str else s)\n",
    "books_ratings_1['title'] = books_ratings_1['title'].replace(r\"\\(.*\\)\",\"\", regex=True)\n",
    "books_ratings_1['title'] = books_ratings_1['title'].replace(r\"\\&\",\"and\", regex=True)\n",
    "\n",
    "#Removes any characters that aren't letters or spaces.\n",
    "books_data_1['title'] = books_data_1['title'].replace(r\"[^a-zA-Z0-9\\s]+\", \"\", regex=True)\n",
    "books_ratings_1['title'] = books_ratings_1['title'].replace(r\"[^a-zA-Z0-9\\s]+\", \"\", regex=True)\n",
    "\n",
    "print(\"Both datasets' titles are now all lowercase and only include letters and spaces.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only unique titles in books_data_1\n",
    "#size of books_data_1\n",
    "prev_size = len(books_data_1['title'])\n",
    "print(f\"Books data has \" + str(len(books_data_1['title'])) + \" titles.\")\n",
    "books_data_1 = books_data_1.drop_duplicates(subset=['title'], keep='first')\n",
    "#books_data_1 = books_data_1.drop_duplicates(subset=['description'], keep='first')\n",
    "print(f\"Dropped \" + str(prev_size - len(books_data_1['title'])) + \" duplicate titles and descriptions. Now there are only \" + str(len(books_data_1['title'])) + \" rows left. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate reviews where same user reviews same book multiple times\n",
    "print(f\"Books ratings has \" + str(len(books_ratings_1['title'])) + \" rows.\")\n",
    "prevRatings = len(books_ratings_1['title'])\n",
    "books_ratings_1.drop_duplicates(keep='first', inplace=True)\n",
    "print(f\"Dropped based on total duplicity. Books ratings now has \" + str(len(books_ratings_1['title'])) + \" rows. \" + \" -\" + str(prevRatings-len(books_ratings_1['title'])) + \" rows.\")\n",
    "prevRatings = len(books_ratings_1['title'])\n",
    "books_ratings_1.drop_duplicates(subset=['user_id', 'title'], keep='first', inplace=True)\n",
    "print(f\"Dropped based on duplicate title. Books ratings now has \" + str(len(books_ratings_1['title'])) + \" rows. \" + \" -\" + str(prevRatings-len(books_ratings_1['title'])) + \" rows.\")\n",
    "prevRatings = len(books_ratings_1['title'])\n",
    "books_ratings_1.drop_duplicates(subset=['user_id', 'text'], keep='first', inplace=True)\n",
    "print(f\"Dropped based on duplicate text. Books ratings now has \" + str(len(books_ratings_1['title'])) + \" rows. \" + \" -\" + str(prevRatings-len(books_ratings_1['title'])) + \" rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove NAN titles\n",
    "print(f\"\" + str(books_ratings_1.isnull().sum().sum()) + \" total NAN values present in title\")\n",
    "books_ratings_1 = books_ratings_1.dropna(subset=['title'])\n",
    "print(f\"Now there are \" + str(books_ratings_1.isnull().sum().sum()))\n",
    "\n",
    "print(f\"Now there are \" + str(books_data_1.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect only ratings for books that have received at least 10 ratings.\n",
    "#Add a new column in books_data_1 that counts the number of ratings for each book with a score of at least 10\n",
    "\n",
    "#Note that by removing duplicate reviews first, counting the number of reviews for each book afterward will be more accurate.\n",
    "print(f\"Books_ratings has \" + str(len(books_ratings_1['title'])) + \" rows.\")\n",
    "prev_rows = len(books_ratings_1['title'])\n",
    "\n",
    "n_ratings = books_ratings_1.groupby('title').count()['score']>=10\n",
    "famous_books = n_ratings[n_ratings==True].index\n",
    "books_ratings_1 = books_ratings_1[books_ratings_1['title'].isin(famous_books)]\n",
    "n_ratings_1 = books_ratings_1['title'].value_counts().rename(\"n_ratings\")\n",
    "books_data_1 = books_data_1.merge(n_ratings_1, on='title', how='inner')\n",
    "\n",
    "print(f\"Books_ratings now has \" + str(len(books_ratings_1['title'])) + \" rows. -\" + str(prev_rows - len(books_ratings_1['title'])) + \" rows. \")\n",
    "print(f\"Books_data now has an additional column. (n_ratings)\")\n",
    "\n",
    "#purge books where n_ratings < 10 (redundant operation)\n",
    "prevBooks = len(books_data_1['title'])\n",
    "books_data_1 = books_data_1[books_data_1['n_ratings']>=10]\n",
    "print(f\"Books data has \" + str(books_data_1.columns.size) + \" columns with \" + str(books_data_1.shape[0]) + \" rows each.\" + \" -\" + str(prevBooks-len(books_data_1['title'])) + \" rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove ratings from users who have rated less than 20 books.\n",
    "prevRatings = len(books_ratings_1['title'])\n",
    "x = books_ratings_1.groupby('user_id').count()['score'] >= 20 # since this is what amazon requires before a user can get recommendations\n",
    "considerable_users = x[x].index\n",
    "books_ratings_1 = books_ratings_1[books_ratings_1['user_id'].isin(considerable_users)]\n",
    "\n",
    "print(f\"Books ratings has \" + str(books_ratings_1.columns.size) + \" columns with \" + str(books_ratings_1.shape[0]) + \" rows each.\" + \" -\" + str(prevRatings-len(books_ratings_1['title'])) + \" rows.\")\n",
    "print(f\"\" + str(len(books_ratings_1['user_id'].value_counts())) + \" users remain who have rated at least 20 books.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column in books_data_1 that calculate  the average score of the book.\n",
    "avg_rating = books_ratings_1.groupby('title')[\"score\"].mean().rename(\"avg_rating\")\n",
    "books_data_1 = books_data_1.merge(avg_rating, on='title', how='inner')\n",
    "print(f\"Books data now has an additional column. (avg_rating)\")\n",
    "\n",
    "# function to get number of users who found the review helpful\n",
    "def get_n_helpful(x):\n",
    "    if isinstance(x, str) and '/' in x:\n",
    "        num, denom = x.split('/')\n",
    "        return int(denom)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# function to get percentage of users who found the review helpful\n",
    "def replace_fraction(x):\n",
    "    if isinstance(x, str) and '/' in x:\n",
    "        num, denom = x.split('/')\n",
    "        if denom == '0':\n",
    "            return 0\n",
    "        else:\n",
    "            return int(num) / int(denom)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# add columns for number of users who found the review helpful and percentage of users who found the review helpful\n",
    "books_ratings_1['helpfulness_count'] = books_ratings_1['helpfulness'].apply(get_n_helpful)\n",
    "books_ratings_1['helpfulness_pct'] = books_ratings_1['helpfulness'].apply(replace_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity on Books Data\n",
    "\n",
    "\n",
    "Function jaccard, that takes two titles and outputs the estimated jaccard similarity. \\\n",
    "Function max_jaccard takes a list of titles and compare the titles with each other. It then returns the two titles with the highest jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(title1, title2):\n",
    "    words1 = set(str(title1).lower().split())\n",
    "    words2 = set(str(title2).lower().split())\n",
    "    \n",
    "    # Compute the intersection and union of the sets\n",
    "    intersection = len(words1.intersection(words2))\n",
    "    union = len(words1) + len(words2) - intersection\n",
    "    \n",
    "    # Calculate the Jaccard similarity\n",
    "    similarity = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def max_jaccard(title_list):\n",
    "    max_similarity = 0.0\n",
    "    idx1 = 0\n",
    "    idx2 = 0\n",
    "    for i in range(len(title_list)):\n",
    "        for j in range(i + 1, len(title_list)):\n",
    "            similarity = jaccard(title_list[i], title_list[j])\n",
    "            if similarity > max_similarity:\n",
    "                idx1 = i\n",
    "                idx2 = j\n",
    "                max_similarity = similarity\n",
    "    return max_similarity,idx1,idx2\n",
    "\n",
    "\"\"\"\n",
    "Example usage for comparing two titles\n",
    "title1 = books_data_1['title'][0]\n",
    "title2 = books_data_1['title'][1]\n",
    "print(title1,title2)\n",
    "similarity = jaccard(title1, title2)\n",
    "print(f\"Jaccard Similarity: {similarity}\")\n",
    "\"\"\"\n",
    "\n",
    "# Example usage with a list of titles\n",
    "n = 100\n",
    "title_list = books_data_1['title'][0:n]\n",
    "max_similarity,idx1,idx2 = max_jaccard(title_list)\n",
    "print(f\"Max Jaccard Similarity: {max_similarity}\")\n",
    "print(books_data_1['title'][idx1])\n",
    "print(books_data_1['title'][idx2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar items using Jaccard similarity\n",
    "\n",
    "Function for finding similar items yusing jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(names, jaccard_threshold=0.6):\n",
    "    # Create a dictionary to store the similar names\n",
    "    similar_names = {}\n",
    "    \n",
    "    # Loop through each name in the list\n",
    "    for i in range(len(names)):\n",
    "        for j in range(i+1, len(names)):\n",
    "            similarity_score = jaccard(names[i], names[j])\n",
    "            if similarity_score >= jaccard_threshold:\n",
    "                similar_names[(names[i], names[j])] = similarity_score\n",
    "    return similar_names\n",
    "\n",
    "# Example usage:    \n",
    "names = books_data_1['title'][0:500]\n",
    "similar_names = similar(names)\n",
    "# Print titles in a way that is easier to read\n",
    "\n",
    "for (desc1, desc2), score in similar_names.items():\n",
    "    print(f\"Similarity Score: {score}\")\n",
    "    print(f\"Title 1: {desc1}\")\n",
    "    print(f\"Title 2: {desc2}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for finding similar descriptions\n",
    "descritions = books_data_1['description'][0:400]\n",
    "similar_descriptions = similar(descritions,jaccard_threshold=0.4)\n",
    "\n",
    "# Print descriptions in a way that is easier to read\n",
    "for (desc1, desc2), score in similar_descriptions.items():\n",
    "    print(f\"Similarity Score: {score}\")\n",
    "    print(f\"Description 1: {desc1}\")\n",
    "    print(f\"Description 2: {desc2}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality-Sensitive Hashing\n",
    "\n",
    "Changing to keys for faster computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "names = books_data_1['title'][0:2]\n",
    "names = names.to_dict()\n",
    "print(names)\n",
    "\n",
    "for key, values in names.items():\n",
    "    print(key)\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split words into k parts\n",
    "def split_k(word,k):\n",
    "    if word is np.nan:\n",
    "        return []\n",
    "    else:\n",
    "        n = len(word)\n",
    "    if k > n:\n",
    "        return split_k(word,int(k/2))\n",
    "    else:\n",
    "        if k == 0:\n",
    "            return word\n",
    "        n1 = n//k\n",
    "        n2 = n1 + n%k\n",
    "        return [word[i:i+n1] for i in range(0, n, n1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the LSH algorithm\n",
    "b = 10\n",
    "\n",
    "def lsh(names, jaccard_threshold,seed):\n",
    "    lsh_dict = {}\n",
    "    for key, name in names.to_dict().items():\n",
    "        blocks = split_k(name,b)\n",
    "        blocks_hash_values = []\n",
    "        for aBlock in blocks:\n",
    "            blocks_hash_values.append(mmh3.hash(aBlock, seed))\n",
    "        lsh_dict[key] = blocks_hash_values\n",
    "    list_keys = list(lsh_dict.keys())\n",
    "    similar_items = {}\n",
    "    for i in range(len(list_keys)-1):\n",
    "        if i% 500 == 0:\n",
    "            print(i)\n",
    "        for j in range(i+1, len(list_keys)):\n",
    "            common_values = np.intersect1d(lsh_dict[list_keys[i]], lsh_dict[list_keys[j]])\n",
    "            if len(common_values) > 0:\n",
    "                # we found a candidate\n",
    "                similarity_score = jaccard(names[list_keys[i]], names[list_keys[j]])\n",
    "                if similarity_score >= jaccard_threshold:\n",
    "                    similar_items[(list_keys[i], list_keys[j])] = similarity_score\n",
    "    return similar_items\n",
    "\n",
    "n = 200\n",
    "\n",
    "\n",
    "titles = books_data_1['title'][0:n]\n",
    "found_similar_items_with_lsh = lsh(titles,jaccard_threshold=0.6,seed=42)\n",
    "# Print the results\n",
    "for (title1, title2), score in found_similar_items_with_lsh.items():\n",
    "    print(f\"Similarity Score: {score}\")\n",
    "    print(f\"Title 1: {titles[title1]}\")\n",
    "    print(f\"Title 2: {titles[title2]}\")\n",
    "    print()\n",
    "    if score == 1:\n",
    "        books_data_1 = books_data_1.drop(title2)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
