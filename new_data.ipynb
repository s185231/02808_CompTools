{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books data has 10 columns with 212404 rows each.\n",
      "Books ratings has 10 columns with 3000000 rows each.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "books_data = pd.read_csv('data/books_data.csv') #, nrows=1000)\n",
    "print(f\"Books data has \" + str(books_data.columns.size) + \" columns with \" + str(books_data.shape[0]) + \" rows each.\")\n",
    "books_ratings = pd.read_csv('data/books_rating.csv') #, nrows=1000)\n",
    "print(f\"Books ratings has \" + str(books_ratings.columns.size) + \" columns with \" + str(books_ratings.shape[0]) + \" rows each.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:29:38.499495600Z",
     "start_time": "2023-11-20T15:28:29.663900500Z"
    }
   },
   "id": "361b23418fb2f778"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books data now only has 5 columns - ['title', 'description', 'authors', 'published_date', 'categories']\n",
      "Books ratings now only has 6 columns - ['title', 'user_id', 'helpfulness', 'score', 'summary', 'text']\n"
     ]
    }
   ],
   "source": [
    "# Titles\n",
    "books_data_1 = books_data.drop(['image', 'previewLink', 'infoLink', 'publisher', 'ratingsCount'], axis=1, inplace=False)\n",
    "books_ratings_1 = books_ratings.drop(['Id','Price', 'profileName', 'review/time'], axis=1, inplace=False)\n",
    "\n",
    "books_data_1.rename(columns={'Title': 'title','publishedDate': 'published_date'}, inplace=True)\n",
    "books_ratings_1.rename(columns={'Title': 'title','User_id':'user_id',  'review/score': 'score','review/helpfulness': 'helpfulness', 'review/text': 'text', 'review/summary': 'summary'}, inplace=True)\n",
    "print(f\"Books data now only has \" + str(books_data_1.columns.size) + \" columns - \" + str(list(books_data_1.columns)))\n",
    "print(f\"Books ratings now only has \" + str(books_ratings_1.columns.size) + \" columns - \" + str(list(books_ratings_1.columns))) \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:14.037650700Z",
     "start_time": "2023-11-20T15:44:29.186153800Z"
    }
   },
   "id": "4e683b0b3a4fef0f"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both datasets' titles are now all lowercase and only include letters and spaces.\n"
     ]
    }
   ],
   "source": [
    "#Makes all titles lowercase; then removes any part of the title that includes a parenthesis, and replaces any ampersands with 'and'\n",
    "books_data_1['title'] = books_data_1['title'].map(lambda s: s.lower() if type(s) == str else s)\n",
    "books_data_1['title'] = books_data_1['title'].replace(r\"\\(.*\\)\",\"\", regex=True)\n",
    "books_data_1['title'] = books_data_1['title'].replace(r\"\\&\",\"and\", regex=True)\n",
    "\n",
    "books_ratings_1['title'] = books_ratings_1['title'].map(lambda s: s.lower() if type(s) == str else s)\n",
    "books_ratings_1['title'] = books_ratings_1['title'].replace(r\"\\(.*\\)\",\"\", regex=True)\n",
    "books_ratings_1['title'] = books_ratings_1['title'].replace(r\"\\&\",\"and\", regex=True)\n",
    "\n",
    "#Removes any characters that aren't letters or spaces.\n",
    "books_data_1['title'] = books_data_1['title'].replace(r\"[^a-zA-Z0-9\\s]+\", \"\", regex=True)\n",
    "books_ratings_1['title'] = books_ratings_1['title'].replace(r\"[^a-zA-Z0-9\\s]+\", \"\", regex=True)\n",
    "\n",
    "print(\"Both datasets' titles are now all lowercase and only include letters and spaces.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:26.228178600Z",
     "start_time": "2023-11-20T15:45:17.056645500Z"
    }
   },
   "id": "28ac36b6874efe74"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books data has 212403 titles.\n",
      "Dropped 7500 duplicate titles. Now there are only 204903 rows left. \n",
      "208 total NAN values present in ratings title\n",
      "Now there are 0\n",
      "0 total NAN values present in data title\n",
      "Now there are 0\n"
     ]
    }
   ],
   "source": [
    "#keep only unique titles in books_data_1\n",
    "#size of books_data_1\n",
    "prev_size = len(books_data_1['title'])\n",
    "print(f\"Books data has \" + str(len(books_data_1['title'])) + \" titles.\")\n",
    "books_data_1 = books_data_1.drop_duplicates(subset=['title'], keep='first')\n",
    "print(f\"Dropped \" + str(prev_size - len(books_data_1['title'])) + \" duplicate titles. Now there are only \" + str(len(books_data_1['title'])) + \" rows left. \")\n",
    "\n",
    "#Remove NAN titles\n",
    "print(f\"\" + str(books_ratings_1['title'].isnull().sum().sum()) + \" total NAN values present in ratings title\")\n",
    "books_ratings_1 = books_ratings_1.dropna(subset=['title'])\n",
    "print(f\"Now there are \" + str(books_ratings_1['title'].isnull().sum().sum()))\n",
    "print(f\"\" + str(books_data_1['title'].isnull().sum().sum()) + \" total NAN values present in data title\")\n",
    "books_data_1 = books_data_1.dropna(subset=['title'])\n",
    "print(f\"Now there are \" + str(books_data_1['title'].isnull().sum().sum()))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:55:26.920319200Z",
     "start_time": "2023-11-20T15:54:54.736857800Z"
    }
   },
   "id": "26aa9480ac40b0af"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books ratings has 2999792 rows.\n",
      "Dropped based on total duplicity. Books ratings now has 2431827 rows.  -567965 rows.\n",
      "Dropped based on duplicate title. Books ratings now has 2027122 rows.  -404705 rows.\n",
      "Dropped based on duplicate text. Books ratings now has 1748097 rows.  -279025 rows.\n",
      "Books data has 163712 rows. -41192 rows.\n"
     ]
    }
   ],
   "source": [
    "#Drop duplicate reviews where same user reviews same book multiple times\n",
    "print(f\"Books ratings has \" + str(len(books_ratings_1['title'])) + \" rows.\")\n",
    "prevRatings = len(books_ratings_1['title'])\n",
    "books_ratings_1.drop_duplicates(keep='first', inplace=True)\n",
    "print(f\"Dropped based on total duplicity. Books ratings now has \" + str(len(books_ratings_1['title'])) + \" rows. \" + \" -\" + str(prevRatings-len(books_ratings_1['title'])) + \" rows.\")\n",
    "prevRatings = len(books_ratings_1['title'])\n",
    "books_ratings_1.drop_duplicates(subset=['user_id', 'title'], keep='first', inplace=True)\n",
    "print(f\"Dropped based on duplicate title. Books ratings now has \" + str(len(books_ratings_1['title'])) + \" rows. \" + \" -\" + str(prevRatings-len(books_ratings_1['title'])) + \" rows.\")\n",
    "prevRatings = len(books_ratings_1['title'])\n",
    "books_ratings_1.drop_duplicates(subset=['user_id', 'text'], keep='first', inplace=True)\n",
    "print(f\"Dropped based on duplicate text. Books ratings now has \" + str(len(books_ratings_1['title'])) + \" rows. \" + \" -\" + str(prevRatings-len(books_ratings_1['title'])) + \" rows.\")\n",
    "\n",
    "#Drop duplicate books where the author and book description are equal.\n",
    "\n",
    "prevBooks = len(books_data_1['title'])\n",
    "books_data_1.drop_duplicates(subset=['description','authors'], keep='first', inplace=True)\n",
    "print(f\"Books data has \" + str(books_data_1.shape[0]) + \" rows.\" + \" -\" + str(prevBooks-len(books_data_1['title'])) + \" rows.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:32:54.075417Z",
     "start_time": "2023-11-12T12:32:41.209989700Z"
    }
   },
   "id": "b09121723c055517"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books_ratings has 1748097 rows.\n",
      "Books_ratings now has 1287419 rows. -460678 rows. \n",
      "Books_data now has an additional column. (n_ratings)\n"
     ]
    }
   ],
   "source": [
    "#Collect only ratings for books that have received at least 10 ratings.\n",
    "#Add a new column in books_data_1 that counts the number of ratings for each book with a score of at least 10\n",
    "#Note that by removing duplicate reviews first, counting the number of reviews for each book afterward will be more accurate.\n",
    "print(f\"Books_ratings has \" + str(len(books_ratings_1['title'])) + \" rows.\")\n",
    "prev_rows = len(books_ratings_1['title'])\n",
    "\n",
    "n_ratings = books_ratings_1.groupby('title').count()['score']>=10\n",
    "famous_books = n_ratings[n_ratings==True].index\n",
    "books_ratings_1 = books_ratings_1[books_ratings_1['title'].isin(famous_books)]\n",
    "n_ratings_1 = books_ratings_1['title'].value_counts().rename(\"n_ratings\")\n",
    "books_data_1 = books_data_1.merge(n_ratings_1, on='title', how='inner')\n",
    "\n",
    "print(f\"Books_ratings now has \" + str(len(books_ratings_1['title'])) + \" rows. -\" + str(prev_rows - len(books_ratings_1['title'])) + \" rows. \")\n",
    "print(f\"Books_data now has an additional column. (n_ratings)\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:33:40.985970Z",
     "start_time": "2023-11-12T12:33:38.881909Z"
    }
   },
   "id": "f30c9015edd25bf1"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books data has 6 columns with 27991 rows each. -0 rows.\n"
     ]
    }
   ],
   "source": [
    "#purge books where n_ratings < 10 (redundant operation)\n",
    "prevBooks = len(books_data_1['title'])\n",
    "books_data_1 = books_data_1[books_data_1['n_ratings']>=10]\n",
    "print(f\"Books data has \" + str(books_data_1.columns.size) + \" columns with \" + str(books_data_1.shape[0]) + \" rows each.\" + \" -\" + str(prevBooks-len(books_data_1['title'])) + \" rows.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:33:44.901239Z",
     "start_time": "2023-11-12T12:33:44.877964300Z"
    }
   },
   "id": "b13ea696aba6df66"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books ratings has 168980 rows. -1118439 rows.\n",
      "3417 users remain who have rated at least 20 books.\n"
     ]
    }
   ],
   "source": [
    "#Remove ratings from users who have rated less than 20 books.\n",
    "prevRatings = len(books_ratings_1['title'])\n",
    "x = books_ratings_1.groupby('user_id').count()['score'] >= 20 # since this is what amazon requires before a user can get recommendations\n",
    "considerable_users = x[x].index\n",
    "books_ratings_1 = books_ratings_1[books_ratings_1['user_id'].isin(considerable_users)]\n",
    "\n",
    "print(f\"Books ratings has \" + str(books_ratings_1.shape[0]) + \" rows.\" + \" -\" + str(prevRatings-len(books_ratings_1['title'])) + \" rows.\")\n",
    "print(f\"\" + str(len(books_ratings_1['user_id'].value_counts())) + \" users remain who have rated at least 20 books.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:33:52.127088300Z",
     "start_time": "2023-11-12T12:33:49.643593300Z"
    }
   },
   "id": "a76984b6b5ece24a"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books data now has an additional column. (avg_rating)\n"
     ]
    }
   ],
   "source": [
    "avg_rating = books_ratings_1.groupby('title')[\"score\"].mean().rename(\"avg_rating\")\n",
    "books_data_1 = books_data_1.merge(avg_rating, on='title', how='inner')\n",
    "print(f\"Books data now has an additional column. (avg_rating)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:34:00.369879200Z",
     "start_time": "2023-11-12T12:34:00.230792200Z"
    }
   },
   "id": "b1a05bb19481306b"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "#convert from string format \"n/d\" to float\n",
    "def get_n_helpful(x):\n",
    "    if isinstance(x, str) and '/' in x:\n",
    "        num, denom = x.split('/')\n",
    "        return int(denom)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def replace_fraction(x):\n",
    "    if isinstance(x, str) and '/' in x:\n",
    "        num, denom = x.split('/')\n",
    "        if denom == '0':\n",
    "            return 0\n",
    "        else:\n",
    "            return int(num) / int(denom)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "books_ratings_1['helpfulness_count'] = books_ratings_1['helpfulness'].apply(get_n_helpful)\n",
    "books_ratings_1['helpfulness_pct'] = books_ratings_1['helpfulness'].apply(replace_fraction)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:34:04.596779800Z",
     "start_time": "2023-11-12T12:34:04.381108600Z"
    }
   },
   "id": "ac61d5cc6189fc85"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First we had 212404 books and 3000000 ratings.\n",
      "Now we have 21294 books and 168980 ratings. -89.97% books and -94.37% ratings.\n"
     ]
    }
   ],
   "source": [
    "#save clean data to file\n",
    "import os\n",
    "if not os.path.exists('clean_data'):\n",
    "    os.makedirs('clean_data')\n",
    "\n",
    "# save clean data\n",
    "books_data_1.to_csv('clean_data/books_data_clean.csv', index=False)\n",
    "books_ratings_1.to_csv('clean_data/books_rating_clean.csv', index=False)\n",
    "\n",
    "prevBooks = len(books_data)\n",
    "prevRatings = len(books_ratings)\n",
    "newBooks = len(books_data_1)\n",
    "newRatings = len(books_ratings_1)\n",
    "\n",
    "print(f\"First we had \" + str(prevBooks) + \" books and \" + str(prevRatings) + \" ratings.\")\n",
    "print(f\"Now we have \" + str(newBooks) + \" books and \" + str(newRatings) + \" ratings. -\" + str(round((prevBooks-newBooks)/prevBooks*100,2)) + \"% books and -\" + str(round((prevRatings-newRatings)/prevRatings*100,2)) + \"% ratings.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:35:00.975113300Z",
     "start_time": "2023-11-12T12:34:52.361270500Z"
    }
   },
   "id": "e6c5f48a3a619416"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
